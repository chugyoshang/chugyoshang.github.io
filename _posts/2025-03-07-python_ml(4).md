---
layout: post
title: python_ml(머신러닝4)
issueNumber: 42
date: 2025-03-07 09:00 +0900
last_modified_at: 2025-03-07 14:00 +0900
categories: [python_ml]
tags: [바이오메디컬, AI, 건양대학교병원, 개발과정, 머신러닝]
toc: true
---
안녕하세요. 이번 포스트에서는 KNN 알고리즘과 K-Means 클러스터링에 대해 정리해보겠습니다! KNN은 데이터 포인트의 최근접 이웃을 찾는 데 사용되며, K-Means는 데이터 클러스터링을 수행하는 대표적인 알고리즘입니다. 이번 포스트에서는 KDTree를 활용한 최근접 이웃 검색부터 KNN 분류 및 회귀, 그리고 이상치 탐지까지 다양한 내용을 다루겠습니다!

1. KDTree를 활용한 최근접 이웃 검색
KDTree를 사용하여 데이터 포인트에 가장 가까운 이웃을 검색.

# 코드 예제:

python
from sklearn.neighbors import KDTree
import numpy as np

data = np.array([[2,3], [5,4], [9,6], [4,7], [8,1], [7,2]])
kdtree = KDTree(data, leaf_size=30)  # 트리 구성
query_point = np.array([[9,2]])     # 검색할 점
distances, indices = kdtree.query(query_point, k=2)

print("Query Point:", query_point)
for i, idx in enumerate(indices[0]):
    print(f"Neighbor {i+1}: {data[idx]}, Distance: {distances[0][i]}")
2. NearestNeighbors를 활용한 KNN
NearestNeighbors를 사용하여 최근접 이웃을 찾고 시각화.

# 코드 예제:

python
import numpy as np
import matplotlib.pyplot as plt
from sklearn.neighbors import NearestNeighbors

X = np.array([[2.1, 1.3], [1.3, 3.2], [2.9, 2.5], [2.7, 5.4], [3.8, 0.9],
              [7.3, 2.1], [4.2, 6.5], [3.8, 3.7], [2.5, 4.1], [3.4, 1.9],
              [5.7, 3.5], [6.1, 4.3], [5.1, 2.2], [6.2, 1.1]])

k =5
test_datapoint = [4.3, 2.7]

knn_model = NearestNeighbors(n_neighbors=k).fit(X)
distances, indices = knn_model.kneighbors([test_datapoint])

plt.scatter(X[:,0], X[:,1], marker='o', s=75)
plt.scatter(X[indices][0][:][:,0], X[indices][0][:][:,1],
            marker='o', s=250, facecolors='none')
plt.scatter(test_datapoint[0], test_datapoint[1], marker='x', s=75)

plt.show()
3. KNN 분류 및 회귀
# KNN 분류:

python
from sklearn.neighbors import KNeighborsClassifier
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split

iris = load_iris()
X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target,
                                                    test_size=0.2,
                                                    random_state=42)
knn_clf = KNeighborsClassifier(n_neighbors=5).fit(X_train, y_train)

y_pred = knn_clf.predict(X_test)
print(f"예측된 클래스: {y_pred}")
print(f"모델 정확도: {knn_clf.score(X_test, y_test):.4f}")
KNN 회귀:

python
from sklearn.neighbors import KNeighborsRegressor
import numpy as np

X = np.array([[1], [2], [3], [4], [5]])
y = np.array([10,12,14,16,18])

knn_reg = KNeighborsRegressor(n_neighbors=3)
knn_reg.fit(X,y)

X_test = np.array([[5]])
y_pred = knn_reg.predict(X_test)

print(f"예측된 값: {y_pred}")
4. 이상치 탐지 (LocalOutlierFactor)
LOF를 사용하여 다변량 이상치를 탐지.

# 코드 예제:

python
from sklearn.neighbors import LocalOutlierFactor
import numpy as np

X = np.random.randn(200,2) * 0.5
outliers = np.random.uniform(low=-4, high=4, size=(20,2))

X = np.vstack([X,outliers])

lof = LocalOutlierFactor(n_neighbors=20)
y_pred = lof.fit_predict(X)

plt.scatter(X[:,0], X[:,1], c=y_pred)
plt.title("이상치 탐지 결과")
plt.show()

결론 및 앞으로의 계획
이번 포스트에서는 KNN 알고리즘과 K-Means 클러스터링의 기초적인 내용을 다뤘습니다. 특히 KDTree를 활용한 최근접 이웃 검색부터 KNN 분류 및 회귀까지 다양한 내용을 이해할 수 있었습니다.

앞으로도 머신러닝의 고급 알고리즘과 다양한 활용 사례를 소개할 예정입니다. 특히 데이터 분석과 이상치 탐지에 관심 있는 분들에게 도움이 되는 자료를 꾸준히 공유하겠습니다.

많은 관심과 피드백 부탁드립니다!