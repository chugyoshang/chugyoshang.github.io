---
layout: post
title: 딥러닝 1일차
issueNumber: 42
date: 2025-03-25 09:00 +0900
last_modified_at: 2025-03-25 14:00 +0900
categories: [딥러닝]
tags: [딥러닝, TensorFlow, 선형대수, AI]
toc: true
---

안녕하세요! 이번 포스트에서는 딥러닝의 기초를 다루는 첫 번째 내용을 소개하겠습니다. 특히 TensorFlow를 활용해 선형대수와 딥러닝에서 자주 사용되는 수학적 개념을 살펴볼 예정입니다. 저도 제 포스트를 보며 복습할 수 있게 차근차근 작성해보겠습니다.
{: .message }

---

## FFNN (Feed Forward Neural Network)

- 행렬곱 연산은 내적 연산의 연속입니다.
- 행렬곱의 의미는 **특징 추출**과 **차원 축소**를 포함합니다.
- 내적 연산의 결과는 크기값이 고려된 **사이각**(벡터의 특징)을 제공합니다.
- 행렬곱은 **정규화**에 유리합니다.

---

## TensorFlow 실행 모드

TensorFlow는 두 가지 실행 모드를 지원합니다.

1. **Static Mode**  
   - 모델을 구성한 후 데이터를 전송하여 실행하는 방식입니다.  
   - 그래프 구조를 미리 생성하기 때문에 속도 면에서 유리합니다.

2. **Dynamic Mode (Eager Mode)**  
   - 함수 실행 시 그래프 구조를 즉시 생성하는 방식입니다.  
   - 개발 편의성이 뛰어나며 PyTorch와 비슷한 느낌으로 작업할 수 있습니다.

---

## GPU 사용 여부 확인

먼저, GPU가 사용 가능한지 확인해 보겠습니다. 아래 코드를 실행하면 현재 시스템에서 GPU를 사용할 수 있는지 알 수 있습니다.

import tensorflow as tf

GPU 사용 가능 여부 확인
print("GPU 사용 가능:", tf.config.list_physical_devices('GPU'))

text

**출력 결과**:
GPU 사용 가능: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]

text

위와 같이 출력된다면 GPU를 활용할 수 있습니다!

---

## TensorFlow 상수 및 변수 선언

TensorFlow에서는 데이터를 저장하기 위해 상수(`constant`)와 변수(`variable`)를 사용할 수 있습니다.

scalar_constant = tf.constant(10) # 스칼라 상수
vector_constant = tf.constant([1,) # 벡터 상수
matrix_constant = tf.constant([[1.0, 2.0], [3.0, 4.0]]) # 행렬 상수

print("스칼라:", scalar_constant)
print("벡터:", vector_constant)
print("행렬:", matrix_constant)

text

**출력 결과**:
스칼라: tf.Tensor(10, shape=(), dtype=int32)
벡터: tf.Tensor([1 2 3], shape=(3,), dtype=int32)
행렬: tf.Tensor(
[[1. 2.]
[3. 4.]], shape=(2, 2), dtype=float32)

text

이처럼 TensorFlow의 `tf.constant()`를 사용해 다양한 형태의 데이터를 정의할 수 있습니다.

---

## 활성화 함수 (Activation Functions)

### ReLU 함수

ReLU(Rectified Linear Unit)는 딥러닝에서 가장 널리 사용되는 활성화 함수 중 하나입니다. 입력 값이 음수일 경우 `0`으로 변환하고 양수일 경우 그대로 반환합니다.

import numpy as np
import matplotlib.pyplot as plt

def relu(x):
return np.maximum(0, x)

x = np.arange(-5.0, 5.0, 0.1)
y = relu(x)

plt.plot(x, y)
plt.title("ReLU Activation Function")
plt.show()

text

**ReLU 함수의 그래프**:
![ReLU 그래프](assets/images/relu.png)

위 그래프는 ReLU 함수의 동작을 시각적으로 보여줍니다. 음수 입력은 모두 `0`으로 처리되며, 양수 입력은 그대로 출력됩니다.

---

## Softmax 함수 구현

Softmax 함수는 여러 클래스 중 하나를 선택하는 확률 값을 계산할 때 사용됩니다.

scores = [3.0, 1.0, 0.2]

def softmax(x):
return np.exp(x) / np.sum(np.exp(x), axis=0)

print(softmax(scores))

text

**출력 결과**:
[0.8360188 , 0.11314284, 0.05083836]

text

Softmax는 각 클래스가 선택될 확률을 계산하며 모든 확률의 합은 `1`이 됩니다.

---

## 결론 및 앞으로의 계획

이번 포스트에서는 TensorFlow를 활용한 선형대수 기초와 딥러닝에서 자주 사용되는 함수들을 살펴보았습니다. 앞으로도 딥러닝 모델 설계와 학습 과정에 대한 내용을 다룰 예정입니다.
{: .message }
궁금한 점이나 추가로 알고 싶은 내용이 있다면 댓글로 남겨주세요! 함께 성장하는 딥러닝 여정을 만들어 갑시다 😊
{: .message }
감사합니다!
{: .message }
