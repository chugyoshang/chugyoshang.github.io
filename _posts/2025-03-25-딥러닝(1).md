---
layout: post
title: 딥러닝 1일차차
issueNumber: 42
date: 2025-03-25 09:00 +0900
last_modified_at: 2025-03-25 14:00 +0900
categories: [딥러닝닝]
tags: [딥러닝, TensorFlow, 선형대수, AI]
toc: true
---
안녕하세요! 이번 포스트에서는 딥러닝의 기초를 다루는 첫 번째 내용을 소개하겠습니다. 특히 TensorFlow를 활용해 선형대수와 딥러닝에서 자주 사용되는 수학적 개념을 살펴볼 예정입니다. 저도 제 포스트를 보며 복습할 수 있게 차근차근 작성해보겠습니다.

---
{: .message }

## FFNN( feed forward neural network )
- 행렬곱 연산은 내적연산의 연속
- 행렬곱의 의미는 특징추출, 차원축소( 복잡 -> 단순 ( 특징 )
- 내적연산의 결과는 크기값이 고려된 사이각 ( 벡터의 특징 )
- 행렬곱은 정규화가 유리
---

## TensorFlow 실행 모드

TensorFlow는 두 가지 실행 모드를 지원합니다.

1. **Static Mode**  
   - 모델을 구성한 후 데이터를 전송하여 실행하는 방식입니다.  
   - 그래프 구조를 미리 생성하기 때문에 속도 면에서 유리합니다.

2. **Dynamic Mode (Eager Mode)**  
   - 함수 실행 시 그래프 구조를 즉시 생성하는 방식입니다.  
   - 개발 편의성이 뛰어나며 PyTorch와 비슷한 느낌으로 작업할 수 있습니다.

---

## GPU 사용 여부 확인

먼저, GPU가 사용 가능한지 확인해 보겠습니다. 아래 코드를 실행하면 현재 시스템에서 GPU를 사용할 수 있는지 알 수 있습니다.
GPU 사용 가능 여부 확인
print("GPU 사용 가능:", tf.config.list_physical_devices('GPU'))

text

**출력 결과**:
GPU 사용 가능: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]

text

위와 같이 출력된다면 GPU를 활용할 수 있습니다!

---

## TensorFlow 상수 및 변수 선언

TensorFlow에서는 데이터를 저장하기 위해 상수(`constant`)와 변수(`variable`)를 사용할 수 있습니다.

scalar_constant = tf.constant(10) # 스칼라 상수
vector_constant = tf.constant([1,) # 벡터 상수
matrix_constant = tf.constant([[1.0, 2.0], [3.0, 4.0]]) # 행렬 상수

print("스칼라:", scalar_constant)
print("벡터:", vector_constant)
print("행렬:", matrix_constant)

text

**출력 결과**:
스칼라: tf.Tensor(10, shape=(), dtype=int32)
벡터: tf.Tensor([1 2 3], shape=(3,), dtype=int32)
행렬: tf.Tensor(
[[1. 2.]
[3. 4.]], shape=(2, 2), dtype=float32)

text

이처럼 TensorFlow의 `tf.constant()`를 사용해 다양한 형태의 데이터를 정의할 수 있습니다.

---

## 고유값 분해 (Eigen Decomposition)

고유값 분해는 행렬의 중요한 특성을 추출하는 데 유용합니다. 아래는 TensorFlow를 이용한 고유값 분해 예제입니다.

a = tf.constant([[3., 1.], [1., 3.]], dtype=tf.float32)
eigenvalues, eigenvectors = tf.linalg.eig(a)

print("고유값:\n", eigenvalues)
print("고유벡터:\n", eigenvectors)

text

**출력 결과**:
고유값:
tf.Tensor([4.+0.j 2.+0.j], shape=(2,), dtype=complex64)
고유벡터:
tf.Tensor(
[[ 0.70710677+0.j -0.70710677+0.j]
[ 0.70710677+0.j 0.70710677+0.j]], shape=(2, 2), dtype=complex64)

text

고유값은 행렬의 특성을 나타내며, 고유벡터는 해당 고유값에 대응하는 방향을 나타냅니다.

---

## 특이값 분해 (Singular Value Decomposition)

특이값 분해(SVD)는 데이터 분석과 차원 축소에 널리 사용됩니다.

a = tf.constant([[1., 2., 3.], [4., 5., 6.]], dtype=tf.float32)
s, u, v = tf.linalg.svd(a)

print("특이값:\n", s)
print("좌직교행렬 U:\n", u)
print("우직교행렬 V:\n", v)

text

**출력 결과**:
특이값:
tf.Tensor([9.508032 0.77286977], shape=(2,), dtype=float32)
좌직교행렬 U:
tf.Tensor(
[[-0.3863177 -0.9223658 ]
[-0.9223658 0.3863177 ]], shape=(2, 2), dtype=float32)
우직교행렬 V:
tf.Tensor(
[[-0.42866713 -0.56630653 -0.70394593]
[-0.80596387 -0.11238241 -0.58119965]
[-0.40824828 -0.8164966 0.40824828]], shape=(3, 3), dtype=float32)

text

SVD는 데이터의 주요 패턴을 파악하거나 차원을 축소하는 데 매우 유용합니다.

---

## 활성화 함수 (Activation Functions)

### ReLU 함수

ReLU(Rectified Linear Unit)는 딥러닝에서 가장 널리 사용되는 활성화 함수 중 하나입니다.

import numpy as np
import matplotlib.pyplot as plt

def relu(x):
return np.maximum(0, x)

x = np.arange(-5.0, 5.0, 0.1)
y = relu(x)

plt.plot(x, y)
plt.title("ReLU Activation Function")
plt.show()

text

ReLU는 입력 값이 음수일 경우 `0`으로 변환하고 양수일 경우 그대로 반환합니다.

---

## Softmax 함수 구현

Softmax 함수는 여러 클래스 중 하나를 선택하는 확률 값을 계산할 때 사용됩니다.

scores = [3.0, 1.0, 0.2]

def softmax(x):
return np.exp(x) / np.sum(np.exp(x), axis=0)

print(softmax(scores))

text

**출력 결과**:
[0.8360188 , 0.11314284, 0.05083836]

text

Softmax는 각 클래스가 선택될 확률을 계산하며 모든 확률의 합은 `1`이 됩니다.

---

## 결론 및 앞으로의 계획

이번 포스트에서는 TensorFlow를 활용한 선형대수 기초와 딥러닝에서 자주 사용되는 함수들을 살펴보았습니다. 앞으로도 딥러닝 모델 설계와 학습 과정에 대한 내용을 다룰 예정입니다.

궁금한 점이나 추가로 알고 싶은 내용이 있다면 댓글로 남겨주세요! 함께 성장하는 딥러닝 여정을 만들어 갑시다 😊

감사합니다!
